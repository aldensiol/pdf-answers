{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "from botocore.config import Config\n",
    "from dotenv import load_dotenv\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "aws_default_region = os.getenv('AWS_DEFAULT_REGION')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "\n",
    "def get_model():\n",
    "    # configuration for retries\n",
    "    retry_config = Config(\n",
    "        region_name='us-east-1',\n",
    "        retries={\n",
    "            'max_attempts': 10,\n",
    "            'mode': 'standard'\n",
    "        }\n",
    "    )\n",
    "    # initialise Bedrock runtime client\n",
    "    bedrock_runtime = boto3.client(\"bedrock-runtime\", config=retry_config)\n",
    "\n",
    "    # model kwargs\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\" # adjust as needed\n",
    "    # model_id =\"anthropic-beta: max-tokens-3-5-sonnet-2024-07-15\"\n",
    "    model_kwargs = {\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 1,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "    }\n",
    "    \n",
    "    llm = ChatBedrock(\n",
    "        client=bedrock_runtime,\n",
    "        model_id=model_id,\n",
    "        model_kwargs=model_kwargs,\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "llm = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_after_references(text):\n",
    "    # Define the substring to search for\n",
    "    marker = \"References\"\n",
    "    \n",
    "    # Find the index of the substring\n",
    "    index = text.find(marker)\n",
    "    \n",
    "    # If the substring is found, slice the string up to the index\n",
    "    if index != -1:\n",
    "        return text[:index + len(marker)]\n",
    "    \n",
    "    # If the substring is not found, return the original text\n",
    "    return text\n",
    "\n",
    "def extract_pdf_text(pdf_path):\n",
    "    # Open the PDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    # Iterate through the pages and extract text\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return remove_after_references(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"pdfs/Project MUSE - Hard Choices.pdf\"\n",
    "pdf_text = extract_pdf_text(PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<system>\n",
    "You are an expert business analyst with deep knowledge of project management, information systems leadership, communication, business innovation, and achieving business objectives. You have extensive experience in analyzing case studies to extract strategic, tactical, and operational insights regarding IT adoption, implementation, and organizational change.\n",
    "</system>\n",
    "\n",
    "<instructions>\n",
    "Given a PDF document containing a case study, perform a thorough analysis and internalize the content. Then, use a step-by-step reasoning process to evaluate the case and solve the query provided.\n",
    "\n",
    "Start by analyzing the details in the PDF and breaking down the information. Identify relevant strategic, operational, and contextual clues. Use these insights to weigh the options in the MCQ and eliminate incorrect ones. Finally, select the best possible answer based on the analysis.\n",
    "\n",
    "After performing the internal reasoning, **only return the correct answer** to the query without showing your intermediate steps or thought process.\n",
    "\n",
    "<query>\n",
    "{query}\n",
    "</query>\n",
    "\n",
    "<pdf_text>\n",
    "{pdf_text}\n",
    "</pdf_text>\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=prompt,\n",
    "    input_variables=[\"query\", \"pdf_text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the case study, the correct answer is:\n",
      "\n",
      "appointed from within KLM\n",
      "\n",
      "The case study mentions that \"The EVP of the Operations Control Centre was appointed as new CIO. It was felt that having the CIO coming out of the 'real business' would help in getting the 'IT governance' discussion out of the IT area, and have it put on the business executive's agenda.\"\n",
      "\n",
      "This indicates that the new CIO was appointed from within KLM, specifically from the Operations Control Centre, rather than being hired through external headhunting.\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"\"\"The new CIO at KLM was \n",
    "\n",
    "appointed from within KLM\n",
    "\n",
    "hired through headhunting\n",
    "\"\"\"\n",
    "\n",
    "MCQ_RESPONSE = chain.invoke({\"query\": QUERY, \"pdf_text\": pdf_text})\n",
    "\n",
    "print(MCQ_RESPONSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
